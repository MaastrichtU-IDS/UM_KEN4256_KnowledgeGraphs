{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì° Introduction to Retrieval Augmented Generation using Knowledge Graphs\n",
    "\n",
    "\n",
    "The **Retrieval Augmented Generation** (RAG) approach involves dynamically retrieving relevant information from a knowledge base to assist in the generation of responses or predictions.\n",
    "\n",
    "In our case, RAG enables to combine structured knowledge from knowledge graphs (KG) with Large Language Models(LLM) natural language capabilities.\n",
    "\n",
    "By default LLMs are 1. a natural language interface with machines; 2. a knowledge base (their training data). But a LLM knowledge base is opaque and not consistent, as the knowledge generated is probabilistic and highly depends on the input ton, it can output contradictory knowledge.\n",
    "\n",
    "With RAG we can reduce the incertainty related to the LLM black-box knowledge base, by plugin an external trusted knowledge base. And using the LLM as a natural language interface to our own data.\n",
    "\n",
    "But it is still a **really new field**. The 2 main approaches currently used are:\n",
    "\n",
    "1. Just vectorize the KG to provide better context to the LLM. Easiest approach (classic RAG):\n",
    "    1. You iterate a KG, generate text embeddings for each concepts (various strategies possible here: text embeddings, GNN...) and store them in a vector database. \n",
    "    2. When a user ask a question to the LLM, you ask the LLM to reformulate the question to make it straight to the point, so the search for relevant concepts in your vector db is more efficient\n",
    "    3. You generate embeddings for this question, and search for the most similar concepts in the vector database\n",
    "    4. You provide the informations about those concepts to the LLM as context, now asking it to answer the question using the provided context.\n",
    "\n",
    "    Its efficiency will depend a lot on the strategy adopted to generate embeddings (do you just use the concepts description? Do you include the links between concepts? How?)\n",
    "\n",
    "2. Use the LLM to generate queries for your KG\n",
    "    1. Get the \"schema\" of the queried KG (e.g. OWL ontology, SHACL shapes, VoID description)\n",
    "    2. Provide the KG schema as input to the LLM\n",
    "    3. Ask the LLM to generate a query to answer your question\n",
    "    4. Execute the query\n",
    "\n",
    "\n",
    "## üéØ Aim\n",
    "\n",
    "This notebook present a demo of **Retrieval Augmented Generation** (RAG) to faithfully resolve and use concepts from an OWL ontology, with conversation memory, using open source components:\n",
    "* [LangChain](https://python.langchain.com) (cf. docs: [RAG with memory](https://python.langchain.com/docs/expression_language/cookbook/retrieval), [streaming RAG](https://python.langchain.com/docs/use_cases/question_answering/streaming))\n",
    "* [FastEmbed embeddings](https://github.com/qdrant/fastembed)\n",
    "* [Qdrant vectorstore](https://github.com/qdrant/qdrant)\n",
    "\n",
    "You can easily change the different components used in this workflow to use whatever you prefer thanks to LangChain: \n",
    "* LLM (e.g. switch to Mistral, and that's it. Every other LLMs are not available in Europe)\n",
    "* Vectorstore (e.g. switch to [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss), [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma), Milvus)\n",
    "* Embedding model (e.g. switch to [HuggingFace sentence transformer](https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers), OpenAI ADA)\n",
    "\n",
    "## üîó Relevant resources\n",
    "\n",
    "Programming frameworks:\n",
    "\n",
    "* [LangChain](https://github.com/langchain-ai/langchain): build linear workflows for LLM\n",
    "* [LangGraph](https://github.com/langchain-ai/langgraph): build circular workflows for LLM, with control loop\n",
    "* [LlamaIndex](): framework to help build RAG systems (a lot of noise around it, but they use global static variables for configuration in their top examples, which is highly concerning for the rest of the code. What if I want to deploy 2 indexes with different config? I can't.)\n",
    "\n",
    "Most existing research for TRAG+KG are using property grap[hs like neo4j or nebula graph (the tooling to work with RDF graphs is too bad):\n",
    "* [KG-RAG](https://github.com/BaranziniLab/KG_RAG): integrate ad-hoc KG data to LLM answers. \n",
    "    1. ask the LLM to extract disease entities from the question\n",
    "    2. query the KG to retrieve the context related to the extracted entities\n",
    "    3. feed the retrieved context to the LLM\n",
    "* [Graph-LLM](https://github.com/CurryTang/Graph-LLM): Paper and code about \"Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs \". Basically LLM + GNN using PyTorch Geometric (PyG). They use LLM to increase the informations used to build the GNN.\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "Your LLM should answer those 2 follow-up questions using the [SemanticScience OWL ontology](https://github.com/MaastrichtU-IDS/semanticscience):\n",
    "\n",
    "- \"What is a protein?\"\n",
    "- \"What is the URI for this concept?\"\n",
    "\n",
    "## üì¶Ô∏è Install and import dependencies\n",
    "\n",
    "Get your OpenAI API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q langchain langchain-community langchain-openai fastembed qdrant-client oxrdflib\n",
    "\n",
    "from operator import itemgetter\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import format_document\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öóÔ∏è Define how to extract infos to embed from the ontology\n",
    "\n",
    "We will extend the LangChain document loader class to create a loader for OWL ontologies\n",
    "\n",
    "**TODO**: you will need to write the SPARQL query to extract the concepts (classes and properties) labels and descriptions from the ontology. \n",
    "\n",
    "The current loader code expect your query to return the variables below.\n",
    "- `?uri` of the concept\n",
    "- `?label` containing the human readable label or description of this concept\n",
    "- `?predicate` defining the retrieved label (e.g. rdfs:label, rdfs:comment, dcterms:description)\n",
    "- `?type` of the concept (e.g. owl:Class or property)\n",
    "\n",
    "Feel free to add more variables! You will just need to add 1 line to the code below to add this variable as concept metadata in the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from rdflib import Graph\n",
    "\n",
    "# TODO: the SPARQL query to retrieve labels and descriptions of classes and properties\n",
    "extract_classes_query = \"\"\"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "PREFIX rdfs:  <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl:  <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?uri ?predicate ?label ?type\n",
    "WHERE {\n",
    "    TODO\n",
    "}\"\"\"\n",
    "\n",
    "class OntologyLoader(BaseLoader):\n",
    "    \"\"\"Load an OWL ontology and extract classes and properties as documents.\"\"\"\n",
    "\n",
    "    def __init__(self, ontology_url: str, format: Optional[str] = None):\n",
    "        self.ontology_url = ontology_url\n",
    "        self.format = format\n",
    "        self.graph = Graph(store=\"Oxigraph\")\n",
    "        self.graph.parse(self.ontology_url, format=self.format)\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load and return documents (classes and properties) from the OWL ontology.\"\"\"\n",
    "        docs: List[Document] = []\n",
    "        for cls in self.graph.query(extract_classes_query):\n",
    "            docs.append(self._create_document(cls))\n",
    "        return docs\n",
    "\n",
    "    def _create_document(self, result_row: Any) -> Document:\n",
    "        \"\"\"Create a Document object from a query result row.\"\"\"\n",
    "        label = str(result_row.label)\n",
    "        return Document(\n",
    "            page_content=label,\n",
    "            # NOTE: you can include more metadata retrieved by the SPARQL query here\n",
    "            metadata={\n",
    "                \"label\": label,\n",
    "                \"uri\": str(result_row.uri),\n",
    "                \"type\": str(result_row.type),\n",
    "                \"predicate\": str(result_row.predicate),\n",
    "                \"ontology\": self.ontology_url,\n",
    "            },\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåÄ Initialize local vectorstore and LLM\n",
    "\n",
    "Use the ontology loader we just created, with the embedding model we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", max_length=512)\n",
    "loader = OntologyLoader(\"https://semanticscience.org/ontology/sio.owl\", format=\"xml\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks if necessary\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    splits,\n",
    "    flag_embeddings,\n",
    "    collection_name=\"ontologies\",\n",
    "    location=\":memory:\",\n",
    "    # path=\"./data/qdrant\",\n",
    ")\n",
    "# vectorstore = FAISS.from_documents(documents=docs, embedding=flag_embeddings)\n",
    "# K is the number of source documents retrieved\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Initialize prompts and memory\n",
    "\n",
    "**TODO**: Fill in the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the memory object that is used to add messages\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# Add a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Prompt to reformulate the question using the chat history\n",
    "reform_template = \"\"\"TODO\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "REFORM_QUESTION_PROMPT = PromptTemplate.from_template(reform_template)\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "answer_template = \"\"\"TODO\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "# Format how the ontology concepts are passed as context to the LLM\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    template=\"Concept label: {page_content} | URI: {uri} | Type: {type} | Predicate: {predicate} | Ontology: {ontology}\"\n",
    ")\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    # print(\"Formatted docs:\", doc_strings)\n",
    "    return document_separator.join(doc_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚õìÔ∏è Define the chain\n",
    "\n",
    "`itemgetter()` is used to retrieve objects defined in the previous step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformulate the question using chat history\n",
    "reformulated_question = {\n",
    "    \"reformulated_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | REFORM_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Retrieve the documents using the reformulated question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"reformulated_question\") | retriever,\n",
    "    \"question\": lambda x: print(\"üí≠ Reformulated question:\", x[\"reformulated_question\"]) or x[\"reformulated_question\"],\n",
    "    # \"question\": lambda x: x[\"reformulated_question\"],\n",
    "}\n",
    "# Construct the inputs for the final prompt using retrieved documents\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# Generate the answer using the retrieved documents and answer prompt\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# Put the chain together\n",
    "final_chain = loaded_memory | reformulated_question | retrieved_documents | answer\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"Ask question, stream the answer output, and return the answer with source documents.\"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        # print(chunk, flush=True)\n",
    "        if \"docs\" in chunk:\n",
    "            output[\"docs\"] = [doc.dict() for doc in chunk[\"docs\"]]\n",
    "            print(\"üìö Documents retrieved:\")\n",
    "            for doc in output[\"docs\"]:\n",
    "                print(f\"¬∑ {doc['page_content']} ({doc['metadata']['uri']})\")\n",
    "            # print(json.dumps(output[\"docs\"], indent=2))\n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"]\n",
    "            print(chunk[\"answer\"], end=\"\", flush=True)\n",
    "    # Add messages to chat history\n",
    "    memory.save_context(inputs, {\"answer\": output[\"answer\"]})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó®Ô∏è Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_debug(True)   # Uncomment to enable detailed LangChain debugging\n",
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"What is a protein?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"What is the URI for this concept?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Going further\n",
    "\n",
    "\n",
    "How could this workflow be improved to answer more complex questions such as:\n",
    "* Give me all properties that can be used for a Protein\n",
    "* Write the SPARQL query to retrieve all human proteins from UniProt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libre-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
